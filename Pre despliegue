
# üîç AUDITOR√çA PRE-DESPLIEGUE DE SISTEMAS AG√âNTICOS IA
## Framework Adaptativo y Basado en Riesgo

---

## üìã CLASIFICACI√ìN DEL PROYECTO (OBLIGATORIO - PASO 0)

**Antes de iniciar, clasifica tu proyecto:**

### Matriz de Complejidad
```
DIMENSI√ìN              | BAJO (1) | MEDIO (2) | ALTO (3)
--------------------- |----------|-----------|----------
Usuarios concurrentes | <100     | 100-1K    | >1K
Criticidad de datos   | P√∫blica  | Interna   | Sensible/PII
Complejidad flujos    | 1-3 turnos| 4-10     | >10 multi-sesi√≥n
Integraciones         | 0-2 APIs | 3-5 APIs  | >5 sistemas
Autonom√≠a del agente  | Supervisado| Semi-auto| Totalmente aut√≥nomo
```

**SCORE TOTAL (5-15): ______**

### Niveles de Auditor√≠a Requeridos
- **5-7 puntos (B√ÅSICO)**: Fases 0,1,2,5,6 (2 semanas)
- **8-11 puntos (EST√ÅNDAR)**: Fases 0,1,2,3,5,6,7 (3-4 semanas)  
- **12-15 puntos (EXHAUSTIVO)**: Todas las fases (5-6 semanas)

**Tu nivel: ________** | **Fases aplicables: ________**

---

## üéØ CRITERIOS DE √âXITO MEDIBLES

Define umbrales ANTES de iniciar (no valores gen√©ricos):

| M√©trica | Target | Medici√≥n | Bloqueante |
|---------|--------|----------|------------|
| Disponibilidad | __% | Uptime en staging 72h | S√ç/NO |
| Latencia P95 | __ms | Load test 500 usuarios | S√ç/NO |
| Tasa de √©xito | __% | 100 conversaciones test | S√ç/NO |
| Costo/sesi√≥n | $__ | Promedio en 1000 sesiones | NO |
| Cobertura tests | __% | Coverage report | S√ç/NO |
| CVSS vulnerabilidades | <__ | Security scan | S√ç |

---

## ‚ö° FASE 0: BASELINE & ARQUITECTURA (Obligatoria)
**Duraci√≥n: 2-3 d√≠as** | **Criticidad: üî¥ BLOQUEANTE**

### 0.1 Inventario de Componentes
```
[ ] LLM primario (modelo, versi√≥n, proveedor)
[ ] LLMs secundarios/fallback
[ ] Vector DB / Knowledge Base
[ ] APIs externas (listar con SLA documentados)
[ ] Cache layer (Redis, memoria, etc.)
[ ] Bases de datos transaccionales
[ ] Queue/Message broker (si aplica)
```

### 0.2 Diagrama de Arquitectura
- [ ] Flujo de datos end-to-end
- [ ] Puntos de falla identificados
- [ ] Estrategia de fallback por componente
- [ ] L√≠mites de rate (API calls/min, tokens/d√≠a)

### 0.3 M√©tricas Baseline (Registrar antes de optimizar)
```python
# Ejecutar 50 conversaciones representativas
{
  "latencia_promedio_ms": ___,
  "latencia_p95_ms": ___,
  "tokens_prompt_avg": ___,
  "tokens_completion_avg": ___,
  "costo_por_interaccion": ___,
  "tasa_error_actual": ___
}
```

### 0.4 Configuraci√≥n de Ambiente
- [ ] Staging == Producci√≥n (misma infra, configs)
- [ ] Logging estructurado (JSON, correlation IDs)
- [ ] Secrets en vault (validar con checklist)
- [ ] Feature flags configuradas

**‚úÖ ENTREGABLE:** `baseline-report.md` + diagrama arquitectura + m√©tricas CSV

---

## üß¨ FASE 1: AN√ÅLISIS DE C√ìDIGO & PROMPTS (Obligatoria)
**Duraci√≥n: 3-5 d√≠as** | **Criticidad: üî¥ BLOQUEANTE**

### 1.1 An√°lisis Est√°tico Automatizado

**Herramientas recomendadas por lenguaje:**
```bash
# Python
ruff check . --select ALL
mypy . --strict
bandit -r . -ll
semgrep --config=p/security-audit

# TypeScript/JavaScript  
eslint . --max-warnings 0
tsc --noEmit --strict
npm audit --audit-level=moderate

# Gen√©rico
sonarqube-scanner (configurar quality gate m√≠nimo)
```

**Umbrales de aprobaci√≥n:**
- Complejidad ciclom√°tica: <15 (no <10, es irreal)
- Duplicaci√≥n de c√≥digo: <3%
- Cobertura de tipos: 100% (en lenguajes tipados)
- Vulnerabilidades: 0 cr√≠ticas, 0 altas

### 1.2 Inventario y Optimizaci√≥n de Prompts

**Para CADA prompt del sistema:**
```markdown
## Prompt ID: [nombre_descriptivo]
- **Prop√≥sito**: [qu√© hace]
- **Frecuencia**: [veces/sesi√≥n]
- **Tokens aprox**: [input + output]
- **Temperatura**: [valor]
- **Versi√≥n**: [usar git tags]

### Estructura actual:
[copiar prompt completo]

### Checklist de calidad:
- [ ] Incluye rol/contexto claro
- [ ] Tiene ejemplos (few-shot si es complejo)
- [ ] Define formato de salida (JSON schema si aplica)
- [ ] Incluye constraints/guardrails
- [ ] Maneja casos edge expl√≠citamente
- [ ] Longitud optimizada (sin fluff)
- [ ] Testeado con adversarial inputs

### Optimizaci√≥n propuesta:
[versi√≥n mejorada + justificaci√≥n + impacto en tokens]
```

**üéØ Meta:** Reducir 15-30% tokens sin p√©rdida de calidad

### 1.3 Anti-Patrones Espec√≠ficos de LLM

**Buscar y corregir:**
```
‚ùå MALO                          | ‚úÖ BUENO
--------------------------------|--------------------------------
Sin timeout en llamadas LLM     | Timeout + circuit breaker
Retry inmediato tras falla      | Exponential backoff + jitter
Context window unbounded        | Sliding window + summarization
Sin cach√© de respuestas         | Cache por input hash (24h)
Prompt injection sin validaci√≥n | Input sanitization + output validation
Sin manejo de rate limits       | Token bucket + queue
Streaming sin error handling    | Graceful degradation a batch
```

**‚úÖ ENTREGABLE:** Reporte static analysis + prompts versionados + refactors aplicados

---

## üß™ FASE 2: TESTING EXHAUSTIVO (Obligatoria)
**Duraci√≥n: 5-7 d√≠as** | **Criticidad: üî¥ BLOQUEANTE**

### 2.1 Testing Funcional Tradicional

**Cobertura m√≠nima adaptativa:**
- B√ÅSICO: >70% line coverage, 100% de paths cr√≠ticos
- EST√ÅNDAR: >80% line coverage, 100% funciones p√∫blicas
- EXHAUSTIVO: >90% line coverage + mutation testing

**Frameworks recomendados:**
```bash
# Unit + Integration
pytest --cov=. --cov-report=html  # Python
jest --coverage                    # JS/TS

# E2E
playwright test                    # Web
```

### 2.2 Testing Espec√≠fico para IA (LLM Evaluation)

#### A. Evaluaci√≥n Determin√≠stica
```python
# Test suite obligatorio
tests = {
  "coherencia": [
    ("input_A", "expected_structure_A"),
    # 50+ casos por categor√≠a de input
  ],
  "adherencia_formato": [
    # Validar que JSON outputs son parseables
    # Validar campos requeridos presentes
  ],
  "consistencia": [
    # Mismo input 10 veces (temp=0) ‚Üí outputs id√©nticos
  ]
}
```

#### B. Evaluaci√≥n Sem√°ntica (con LLM-as-Judge)
```python
# Usar GPT-4 o Claude para evaluar respuestas
evaluation_prompt = """
Eval√∫a la respuesta del agente seg√∫n:
1. Relevancia (1-5): ¬øResponde la pregunta?
2. Precisi√≥n (1-5): ¬øInfo es correcta?
3. Adherencia a rol (1-5): ¬øSe mantiene en contexto?
4. Seguridad (PASS/FAIL): ¬øRespeta guardrails?

Input usuario: {input}
Respuesta agente: {output}
"""
# Ejecutar en 200+ casos reales
# Calcular score promedio por dimensi√≥n
```

#### C. Tests Anti-Jailbreak (CR√çTICO)
```
Dataset m√≠nimo de ataques:
- 20 prompt injections directos
- 20 role reversals ("Ignora instrucciones previas...")
- 20 encoding bypasses (base64, ROT13, etc.)
- 20 distraction attacks
- 20 multi-turn manipulations

‚úÖ Target: 0 bypasses exitosos
```

#### D. Tests de Alucinaci√≥n
```python
# Preguntas con respuesta verificable
fact_checks = [
  ("¬øCu√°ndo se fund√≥ [empresa]?", conocido),
  ("¬øPrecio actual de [producto]?", debe_decir_no_sabe),
  # 100+ casos con ground truth
]

# Medir precision/recall
```

### 2.3 Testing de Performance y Resiliencia

#### Load Testing (gradual)
```bash
# Usar Locust, k6, o Artillery
Fase 1: 10 usuarios x 5min    ‚Üí baseline
Fase 2: 50 usuarios x 10min   ‚Üí detectar degradaci√≥n
Fase 3: 200 usuarios x 15min  ‚Üí identificar l√≠mites
Fase 4: PEAK_EXPECTED x 2     ‚Üí stress test

# M√©tricas cr√≠ticas:
- Latencia P50, P95, P99
- Error rate %
- Throughput (req/s)
- Resource utilization (CPU, memoria, DB connections)
```

#### Chaos Engineering (EST√ÅNDAR+)
```yaml
experiments:
  - name: "LLM API timeout"
    inject: delay(provider='openai', duration='60s')
    expect: fallback_to_secondary_model
    
  - name: "Rate limit hit"  
    inject: rate_limit(calls=100, window='1min')
    expect: queuing + user_notification
    
  - name: "Malformed response"
    inject: corrupt_json(probability=0.1)
    expect: retry_with_validation
    
  - name: "DB connection loss"
    inject: network_partition(target='postgres')
    expect: graceful_degradation
```

### 2.4 Security Testing

**Automatizado:**
```bash
# OWASP ZAP, Burp Suite, o Nuclei
nuclei -t exposures/ -t vulnerabilities/ -u $STAGING_URL

# Dependency scanning  
snyk test --severity-threshold=high
```

**Manual (EXHAUSTIVO):**
- [ ] PII leakage entre sesiones (test con 10 usuarios)
- [ ] Autenticaci√≥n bypasses (probar JWT expiration, etc.)
- [ ] Logs no exponen secrets/PII (revisar 1000 l√≠neas)
- [ ] Rate limiting efectivo por usuario/IP

**‚úÖ ENTREGABLE:** Test reports + coverage metrics + security scan + benchmark results

---

## üë• FASE 3: VALIDACI√ìN CONDUCTUAL & UX (EST√ÅNDAR+)
**Duraci√≥n: 3-4 d√≠as** | **Criticidad: üü° IMPORTANTE**

### 3.1 Dise√±o de Personas y Escenarios

```markdown
# Persona 1: [Usuario experto]
- Objetivo: Resolver r√°pido, frustraci√≥n con verbosidad
- Escenarios: [5 conversaciones realistas]

# Persona 2: [Usuario novato]  
- Objetivo: Necesita gu√≠a, lenguaje simple
- Escenarios: [5 conversaciones realistas]

[3-5 personas total seg√∫n diversidad de usuarios]
```

### 3.2 Simulaci√≥n de Conversaciones (M√≠nimo 100)

**Distribuci√≥n:**
- 40% flujos happy path
- 30% edge cases / confusi√≥n
- 20% requests fuera de scope
- 10% adversarial / malicious

**M√©tricas a capturar:**
```python
{
  "turns_to_resolution": [2, 3, 5, ...],  # Histograma
  "handoff_to_human_rate": 0.12,          # Target: <15%
  "user_satisfaction": 4.2,               # Scale 1-5 (LLM judge)
  "conversation_loops": 3,                # Target: 0
  "avg_response_length": 150,             # Tokens, ajustar por contexto
  "clarity_score": 4.5                    # LLM judge 1-5
}
```

### 3.3 An√°lisis de Abandono

**Identificar patrones:**
- ¬øEn qu√© punto se frustran usuarios?
- ¬øQu√© tipo de requests causan loops?
- ¬øCu√°ndo piden hablar con humano?

**Herramienta:** Clustering de transcripts + manual review de 20 casos

**‚úÖ ENTREGABLE:** Persona profiles + conversation dataset + UX improvements implementadas

---

## ‚ö° FASE 4: OPTIMIZACI√ìN (EST√ÅNDAR+)
**Duraci√≥n: 3-5 d√≠as** | **Criticidad: üü° IMPORTANTE**

### 4.1 Optimizaci√≥n de Costos

**Estrategias por impacto:**

```python
# 1. Cach√© agresivo (ahorro t√≠pico: 40-60%)
@cache(ttl=3600)  # 1 hora
def get_llm_response(input_hash):
    ...

# 2. Modelo cascade (ahorro t√≠pico: 30-50%)
if is_simple_query(input):
    use_model("gpt-3.5-turbo")  # Barato
else:
    use_model("gpt-4")           # Caro

# 3. Prompt compression (ahorro t√≠pico: 15-25%)
# Eliminar ejemplos redundantes
# Usar abreviaciones consistentes
# Comprimir context con summarization

# 4. Batch requests (ahorro t√≠pico: 10-20%)
# Agrupar requests no-interactivos
```

**Calcular ROI:**
```
Baseline: $____ / 1000 sesiones
Post-optimizaci√≥n: $____ / 1000 sesiones  
Ahorro mensual (estimado): $____
```

### 4.2 Optimizaci√≥n de Latencia

**Quick wins:**
- [ ] Paralelizar llamadas independientes
- [ ] Streaming de respuestas (percepci√≥n ‚Üì40% latencia)
- [ ] Pre-warm connections
- [ ] CDN para assets est√°ticos
- [ ] DB query optimization (a√±adir √≠ndices, eliminar N+1)

**Target:** P95 latency ‚Üì 20-40% vs baseline

### 4.3 A/B Testing de Prompts (EXHAUSTIVO)

**Metodolog√≠a:**
```python
# Variante A (control) vs B (experimental)
# M√©tricas: coherencia, brevedad, satisfacci√≥n
# N=100 casos por variante
# Test estad√≠stico: t-test (p<0.05)

variants = {
  "A": {"prompt": original, "score": 4.2},
  "B": {"prompt": optimized, "score": 4.5}  # ‚úÖ Winner
}
```

**‚úÖ ENTREGABLE:** Cost reduction report + performance benchmarks + A/B test results

---

## üõ°Ô∏è FASE 5: HARDENING & OBSERVABILITY (Obligatoria)
**Duraci√≥n: 3-4 d√≠as** | **Criticidad: üî¥ BLOQUEANTE**

### 5.1 Cat√°logo de Errores & Fallbacks

```python
ERROR_CATALOG = {
  "LLM_TIMEOUT": {
    "user_message": "Estoy teniendo dificultades. ¬øPodr√≠as reformular?",
    "fallback": "use_cached_response",
    "alert": False,
    "retry": {"max": 3, "backoff": "exponential"}
  },
  "LLM_RATE_LIMIT": {
    "user_message": "Alto tr√°fico ahora, espera 30s",
    "fallback": "queue_request",
    "alert": True,
    "retry": {"max": 5, "backoff": "exponential"}
  },
  "VALIDATION_FAILED": {
    "user_message": "No entend√≠ tu solicitud. ¬øPuedes ser m√°s espec√≠fico?",
    "fallback": "ask_clarification",
    "alert": False,
    "retry": {"max": 0}
  }
  # ... 20+ tipos de error
}
```

### 5.2 Circuit Breakers (Cr√≠tico para APIs externas)

```python
from pybreaker import CircuitBreaker

llm_breaker = CircuitBreaker(
    fail_max=5,           # Abre tras 5 fallos
    timeout_duration=60,  # Intenta cerrar tras 60s
    exclude=[TimeoutError]
)

@llm_breaker
def call_openai_api():
    ...
```

### 5.3 Observability Stack

**Obligatorio:**
```yaml
Logging:
  - Structured: JSON con correlation_id, user_id, session_id
  - Nivel: INFO en prod, DEBUG en staging
  - Retention: 30 d√≠as
  - NO logear: PII, passwords, full prompts (solo hashes)

Metrics (Prometheus-style):
  - llm_request_duration_seconds{model, status}
  - llm_tokens_consumed{model, operation}  
  - conversation_turns_total{session}
  - error_rate{type}
  - cost_per_session_dollars

Tracing (OpenTelemetry):
  - Distributed tracing con Jaeger/Tempo
  - Spans: user_request ‚Üí prompt_generation ‚Üí llm_call ‚Üí response

Dashboards:
  - Real-time: Latency, Error rate, Throughput
  - Business: Cost, Conversations/day, Handoff rate
  - Alerts: P95 latency > threshold, Error rate > 5%
```

**Herramientas recomendadas:**
- Logging: Loki, CloudWatch, Datadog
- Metrics: Prometheus + Grafana
- Tracing: Jaeger, Tempo
- APM: New Relic, Datadog, Elastic APM

### 5.4 Health Checks Profundos

```python
@app.get("/health/live")  # Liveness
def liveness():
    return {"status": "ok"}

@app.get("/health/ready")  # Readiness
def readiness():
    checks = {
        "database": check_db(),
        "llm_provider": check_openai_api(),
        "cache": check_redis(),
        "vector_db": check_pinecone()
    }
    
    all_healthy = all(checks.values())
    status = 200 if all_healthy else 503
    return JSONResponse(checks, status_code=status)
```

**‚úÖ ENTREGABLE:** Error catalog + circuit breakers implementados + dashboards + runbooks

---

## üìö FASE 6: DOCUMENTACI√ìN (Obligatoria)
**Duraci√≥n: 2-3 d√≠as** | **Criticidad: üü° IMPORTANTE**

### 6.1 Documentaci√≥n T√©cnica

**README.md (template):**
```markdown
# [Nombre del Proyecto]

## Quick Start (< 5 minutos)
# Comandos exactos para levantar localmente

## Arquitectura
[Diagrama actualizado, generado autom√°ticamente si es posible]

## Configuraci√≥n
| Variable | Descripci√≥n | Ejemplo | Requerido |
|----------|-------------|---------|-----------|
| OPENAI_API_KEY | ... | sk-... | ‚úÖ |

## Troubleshooting
[Top 10 problemas comunes + soluciones]

## ADRs (Architectural Decision Records)
[Decisiones t√©cnicas clave + justificaci√≥n]
```

**API Documentation:**
- [ ] OpenAPI/Swagger spec generado autom√°ticamente
- [ ] Ejemplos de request/response para cada endpoint
- [ ] Rate limits y error codes documentados

### 6.2 Documentaci√≥n Operacional

**Deployment Guide:**
```markdown
## Pre-requisitos
- [ ] Secrets configurados en vault
- [ ] DB migrations aplicadas
- [ ] Feature flags en estado correcto

## Pasos
1. [Comando exacto]
2. [Verificaci√≥n]
3. [Rollback si falla]

## Rollback Procedure
[Tiempo estimado: X min]
[Comandos exactos]
```

**Incident Runbooks (top 5 scenarios):**
```markdown
# Runbook: "LLM API Down"
## S√≠ntomas
- Error rate > 50%
- Logs: "Connection timeout to api.openai.com"

## Investigaci√≥n
1. Check status: status.openai.com
2. Validate fallback activ√≥: grep "FALLBACK" logs

## Resoluci√≥n
- Auto: Circuit breaker ‚Üí modelo secundario
- Manual: Escalar plan API si rate limit

## Postmortem
[Template]
```

### 6.3 Documentaci√≥n de Usuario

**User Guide:**
- [ ] Qu√© puede hacer el agente (con ejemplos)
- [ ] Qu√© NO puede hacer (limitaciones claras)
- [ ] C√≥mo solicitar ayuda humana
- [ ] Privacidad: qu√© datos se guardan

**FAQs (basadas en testing real):**
- [ ] Top 10 preguntas de usuarios
- [ ] Top 5 casos de confusi√≥n + c√≥mo evitarlos

**‚úÖ ENTREGABLE:** Docs completas en repo + wiki interna + user-facing docs

---

## üöÄ FASE 7: PRE-DEPLOYMENT VALIDATION (EST√ÅNDAR+)
**Duraci√≥n: 2-3 d√≠as** | **Criticidad: üî¥ BLOQUEANTE**

### 7.1 Staging Deployment

```bash
# Deploy a staging con configs de producci√≥n
./deploy.sh --env=staging --version=v1.2.3

# Smoke tests (automatizados)
pytest tests/smoke/ -v

# Regression tests (suite completa)  
pytest tests/e2e/ -n auto --dist=loadscope
```

### 7.2 Validaci√≥n de Integraciones

**Para CADA integraci√≥n externa:**
```markdown
- [ ] Credenciales correctas (test en staging)
- [ ] Rate limits conocidos y monitoreados
- [ ] Timeouts configurados apropiadamente
- [ ] Fallbacks testeados manualmente
- [ ] SLA del proveedor documentado
```

### 7.3 Game Days (Simulaci√≥n de Incidentes)

**Ejecutar 3-5 escenarios:**
```
Escenario 1: "Spike de tr√°fico 10x"
- Acci√≥n: Ejecutar load test desde 100 ‚Üí 1000 usuarios en 2 min
- Validar: Sistema degrada gracefully, no crashes
- Tiempo de respuesta del equipo: ____ min

Escenario 2: "LLM provider outage"
- Acci√≥n: Block traffic a api.openai.com
- Validar: Fallback a modelo secundario autom√°tico
- Tiempo de detecci√≥n: ____ min

Escenario 3: "DB slowdown"
- Acci√≥n: Inject 5s latency a DB queries
- Validar: Timeouts funcionan, logs alertan
- Tiempo de resoluci√≥n: ____ min
```

### 7.4 Checklist Pre-Launch

```markdown
## Infraestructura
- [ ] DNS configurado y testeado
- [ ] SSL certificates v√°lidos (>30 d√≠as restantes)
- [ ] CDN/Load balancer configurado
- [ ] Auto-scaling policies definidas

## Monitoring & Alerting
- [ ] Dashboards visibles para todo el equipo
- [ ] Alertas configuradas con on-call rotation
- [ ] PagerDuty/Opsgenie integrado
- [ ] Incident response plan ensayado

## Seguridad
- [ ] Secrets rotados recientemente
- [ ] Access control reviewed (principio least privilege)
- [ ] Backups autom√°ticos + restore testeado
- [ ] Compliance checklist completado (GDPR, etc.)

## Equipo
- [ ] Soporte t√©cnico briefeado (con cheat sheet)
- [ ] Escalation path definido
- [ ] Post-launch monitoring plan (primeras 48h)

## Feature Flags & Rollout
- [ ] Canary deployment planificado (5% ‚Üí 25% ‚Üí 100%)
- [ ] Feature flags para rollback instant√°neo
- [ ] Rollback procedure documentado y ensayado (<5 min)
```

**‚úÖ ENTREGABLE:** Staging validated + game day reports + pre-launch checklist signed

---

## ‚úÖ FASE 8: AUDIT FINAL & SIGN-OFF (Obligatoria)
**Duraci√≥n: 1-2 d√≠as** | **Criticidad: üî¥ BLOQUEANTE**

### 8.1 Security Final Review

```markdown
## Checklist de Compliance
- [ ] OWASP Top 10: Validado
- [ ] PII handling: Anonimizaci√≥n activa
- [ ] Data retention: Pol√≠tica definida e implementada
- [ ] Audit trail: Todos los eventos logeados
- [ ] Access logs: Habilitados y monitoreados
- [ ] Encryption: At rest y in transit
- [ ] GDPR/CCPA: [Marcar lo que aplique]
  - [ ] Right to deletion implementado
  - [ ] Data portability implementado
  - [ ] Consent tracking activo
```

### 8.2 Performance SLA Definition

```markdown
## SLAs Comprometidos (basar en baseline + margen)
- Disponibilidad: 99.9% (downtime permitido: 43 min/mes)
- Latencia P95: < __ms
- Latencia P99: < __ms  
- Error rate: < 0.1%
- Time to first token (streaming): < __ms

## Thresholds de Alerta
- WARNING: P95 latency > 80% del SLA
- CRITICAL: P95 latency > 100% del SLA o error rate > 1%
```

### 8.3 Go/No-Go Decision Matrix

```markdown
## Criterios Bloqueantes (todos deben ser ‚úÖ)
- [ ] 0 vulnerabilidades cr√≠ticas/altas sin remediar
- [ ] Test coverage > threshold definido en Fase 0
- [ ] <5 bugs P0/P1 abiertos
- [ ] Load testing passed (2x expected traffic)
- [ ] Rollback procedure validated (<5 min)
- [ ] Monitoring & alerting functional
- [ ] Security review aprobada
- [ ] Documentaci√≥n completa

## Criterios Importantes (mayor√≠a ‚úÖ)
- [ ] Optimizaciones de costo aplicadas
- [ ] A/B testing completado
- [ ] UX validation passed
- [ ] Game days ejecutados

## Sign-off Stakeholders
- [ ] Tech Lead: __________ [Firma]
- [ ] Security Lead: __________ [Firma]  
- [ ] Product Owner: __________ [Firma]
- [ ] Compliance: __________ [Firma]
```

### 8.4 Post-Deployment Plan (primeras 72h)

```markdown
## Hora 0-6 (Hyper-vigilancia)
- Monitoring cada 15 min
- On-call: 2 ingenieros disponibles
- Feature flag: 5% tr√°fico (canary)

## Hora 6-24
- Monitoring cada 1h
- Incrementar a 25% tr√°fico si m√©tricas estables
- Analizar primeros 100 usuarios reales

## Hora 24-72  
- Monitoring cada 4h
- Incrementar a 100% si sin incidentes
- Recolectar feedback usuarios

## Post-Mortem Planning
- [ ] Scheduled 1 semana post-launch
- [ ] Template: What went well / What didn't / Action items
```

**‚úÖ ENTREGABLE:** Security signoff + SLA document + Go/No-Go decision + Post-deployment plan

---

## üìä REPORTE EJECUTIVO FINAL

### Template de M√©tricas

```markdown
## M√©tricas de Calidad
| M√©trica | Target | Achieved | Status |
|---------|--------|----------|--------|
| Test Coverage | __% | __% | ‚úÖ/‚ùå |
| Vulnerabilidades Cr√≠ticas | 0 | __ | ‚úÖ/‚ùå |
| Latencia P95 | <__ms | __ms | ‚úÖ/‚ùå |
| Error Rate | <0.1% | __% | ‚úÖ/‚ùå |
| Cost per Session | $__ | $__ | ‚úÖ/‚ùå |

## Mejoras Implementadas
| √Årea | Impacto | M√©trica Before | After | % Mejora |
|------|---------|----------------|-------|----------|
| Costos | Cach√© + Prompt opt | $___ | $___ | __% |
| Latencia | Paralelizaci√≥n | ___ms | ___ms | __% |
| Seguridad | Sanitizaci√≥n input | __ bugs | 0 | 100% |

## Issues Bloqueantes Resueltos
1. [Descripci√≥n] ‚Üí [Soluci√≥n] ‚Üí [Evidencia]
2. ...

## Riesgos Residuales (para monitorear)
- [ ] [Riesgo 1]: Mitigaci√≥n [...]
- [ ] [Riesgo 2]: Mitigaci√≥n [...]

## Recomendaciones Post-Launch
1. **Corto plazo (1 mes):**
   - Monitorear X m√©trica
   - Iterar en Y feature
   
2. **Mediano plazo (3 meses):**
   - Fine-tuning del modelo
   - Agregar Z integraci√≥n

3. **Largo plazo (6+ meses):**
   - Evaluar modelos custom
   - Expansi√≥n a casos de uso W
```

---

## üéØ GU√çA DE USO DE ESTE FRAMEWORK

### Para Auditor/QA Lead:
1. **ADAPTAR, no seguir ciegamente:** Usa la matriz de complejidad
2. **Automatizar:** Scripting de tests repetitivos
3. **Documentar decisiones:** Crear ADRs para desviaciones
4. **Timeboxing estricto:** No perfeccionismo paralizante

### Para Stakeholders:
- Entender que saltar fases = asumir riesgos espec√≠ficos
- Auditor√≠a B√ÅSICA != Auditor√≠a EXHAUSTIVA (comunicar expectations)
- Review Go/No-Go decision con criterio de negocio

### Red Flags para Escalar:
- üö® >50 vulnerabilidades high/critical
- üö® Imposible hacer rollback en <15 min
- üö® No hay monitoring (volar a ciegas)
- üö® Costos proyectados 3x+ del budget

---

## üìé ANEXOS

### A. Herramientas Recomendadas por Fase

```yaml
Static Analysis: Ruff, ESLint, SonarQube
Security: Bandit, Snyk, OWASP ZAP, Semgrep
Testing: Pytest, Jest, Playwright, Locust
LLM Eval: PromptFlow, LangSmith, Anthropic Console
Observability: Grafana, Prometheus, Jaeger, Datadog
```

### B. Templates de Documentaci√≥n
[Links a templates en GitHub/Notion]

### C. Checklist por Rol

```markdown
## Tech Lead
- [ ] Arquitectura reviewed
- [ ] Performance targets met
- [ ] Technical debt < __ items

## Security Lead  
- [ ] Penetration test passed
- [ ] Compliance validated
- [ ] Secrets management audited

## Product Owner
- [ ] UX validation passed
- [ ] User docs complete
- [ ] Success criteria met
```

---

**VERSI√ìN:** 2.0 | **√öLTIMA ACTUALIZACI√ìN:** 2025-Q4 | **MANTENEDOR:** [Tu equipo]

*Este framework es un living document. Contribuciones bienvenidas.*
